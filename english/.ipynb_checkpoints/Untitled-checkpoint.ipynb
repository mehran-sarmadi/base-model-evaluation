{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf5721e6-cb42-4993-ac52-ff9f34578b95",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'english'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset, DatasetDict, Dataset\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menglish\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer_qa\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QuestionAnsweringTrainer\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01menglish\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils_qa\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m postprocess_qa_predictions\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     AutoModelForQuestionAnswering,\n\u001b[1;32m      9\u001b[0m     AutoTokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     set_seed,\n\u001b[1;32m     14\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'english'"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "from english.trainer_qa import QuestionAnsweringTrainer\n",
    "from english.utils_qa import postprocess_qa_predictions\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "metric = evaluate.load(\"squad\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    return metric.compute(predictions=eval_pred.predictions, references=eval_pred.label_ids)\n",
    "\n",
    "\n",
    "raw_datasets = load_dataset(\"rajpurkar/squad\")\n",
    "train_test_split = raw_datasets['train'].train_test_split(test_size = 0.01, seed=42)    \n",
    "raw_datasets['train'] = train_test_split['train'].select(list(range(1, 8000)))\n",
    "raw_datasets['validation'] =  raw_datasets['validation'].select(list(range(1000, 1700)))\n",
    "raw_datasets['test'] =  train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d764b1c2-8c8b-499d-a12e-0f823b81a4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def squad_main(model_name_or_path, logger, output_dir, tokenizer_name_or_path=None):\n",
    "    logger.info(\"Loading dataset...\")\n",
    "\n",
    "\n",
    "    if tokenizer_name_or_path is None:\n",
    "        logger.info(\"Loading tokenizer from %s\", model_name_or_path)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "    else:\n",
    "        logger.info(\"Loading tokenizer from %s\", tokenizer_name_or_path)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "\n",
    "    logger.info(\"Loading model from %s\", model_name_or_path)\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_name_or_path)\n",
    "    \n",
    "\n",
    "    column_names = raw_datasets[\"train\"].column_names\n",
    "    question_column_name = \"question\" if \"question\" in column_names else column_names[0]\n",
    "    context_column_name = \"context\" if \"context\" in column_names else column_names[1]\n",
    "    answer_column_name = \"answers\" if \"answers\" in column_names else column_names[2]\n",
    "\n",
    "    pad_on_right = tokenizer.padding_side == \"right\"\n",
    "\n",
    "\n",
    "    def prepare_train_features(examples):\n",
    "        # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "        # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "        # left whitespace\n",
    "        examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n",
    "\n",
    "        # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "        # in one example possible giving several features when a context is long, each of those features having a\n",
    "        # context that overlaps a bit the context of the previous feature.\n",
    "        tokenized_examples = tokenizer(\n",
    "            examples[question_column_name if pad_on_right else context_column_name],\n",
    "            examples[context_column_name if pad_on_right else question_column_name],\n",
    "            truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "            max_length=512,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "        # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "        # its corresponding example. This key gives us just that.\n",
    "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "        # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "        # help us compute the start_positions and end_positions.\n",
    "        offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "        # Let's label those examples!\n",
    "        tokenized_examples[\"start_positions\"] = []\n",
    "        tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "        for i, offsets in enumerate(offset_mapping):\n",
    "            # We will label impossible answers with the index of the CLS token.\n",
    "            input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "            if tokenizer.cls_token_id in input_ids:\n",
    "                cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "            elif tokenizer.bos_token_id in input_ids:\n",
    "                cls_index = input_ids.index(tokenizer.bos_token_id)\n",
    "            else:\n",
    "                cls_index = 0\n",
    "\n",
    "            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "            # One example can give several spans, this is the index of the example containing this span of text.\n",
    "            sample_index = sample_mapping[i]\n",
    "            answers = examples[answer_column_name][sample_index]\n",
    "            # If no answers are given, set the cls_index as answer.\n",
    "            if len(answers[\"answer_start\"]) == 0:\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Start/end character index of the answer in the text.\n",
    "                start_char = answers[\"answer_start\"][0]\n",
    "                end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "                # Start token index of the current span in the text.\n",
    "                token_start_index = 0\n",
    "                while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                    token_start_index += 1\n",
    "\n",
    "                # End token index of the current span in the text.\n",
    "                token_end_index = len(input_ids) - 1\n",
    "                while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                    token_end_index -= 1\n",
    "\n",
    "                # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                    tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                    tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "                else:\n",
    "                    # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                    # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                        token_start_index += 1\n",
    "                    tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                    while offsets[token_end_index][1] >= end_char:\n",
    "                        token_end_index -= 1\n",
    "                    tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "        return tokenized_examples\n",
    "        # Validation preprocessing\n",
    "    def prepare_validation_features(examples):\n",
    "        # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "        # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "        # left whitespace\n",
    "        examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n",
    "\n",
    "        # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "        # in one example possible giving several features when a context is long, each of those features having a\n",
    "        # context that overlaps a bit the context of the previous feature.\n",
    "        tokenized_examples = tokenizer(\n",
    "            examples[question_column_name if pad_on_right else context_column_name],\n",
    "            examples[context_column_name if pad_on_right else question_column_name],\n",
    "            truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "            max_length=512,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "\n",
    "        # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "        # its corresponding example. This key gives us just that.\n",
    "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "        # For evaluation, we will need to convert our predictions to substrings of the context, so we keep the\n",
    "        # corresponding example_id and we will store the offset mappings.\n",
    "        tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "        for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "            context_index = 1 if pad_on_right else 0\n",
    "\n",
    "            # One example can give several spans, this is the index of the example containing this span of text.\n",
    "            sample_index = sample_mapping[i]\n",
    "            tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "            # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "            # position is part of the context or not.\n",
    "            tokenized_examples[\"offset_mapping\"][i] = [\n",
    "                (o if sequence_ids[k] == context_index else None)\n",
    "                for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "            ]\n",
    "\n",
    "        return tokenized_examples\n",
    "\n",
    "    logger.info(\"Tokenizing dataset...\")\n",
    "    train_dataset = raw_datasets[\"train\"].map(\n",
    "                prepare_train_features,\n",
    "                batched=True,\n",
    "                num_proc=8,\n",
    "                remove_columns=column_names,\n",
    "                desc=\"Running tokenizer on train dataset\",\n",
    "            )\n",
    "\n",
    "    eval_dataset = raw_datasets[\"validation\"].map(\n",
    "                prepare_validation_features,\n",
    "                batched=True,\n",
    "                num_proc=8,\n",
    "                remove_columns=column_names,\n",
    "                desc=\"Running tokenizer on validation dataset\",\n",
    "            )\n",
    "    test_dataset = raw_datasets[\"test\"].map(\n",
    "                prepare_validation_features,\n",
    "                batched=True,\n",
    "                num_proc=8,\n",
    "                remove_columns=column_names,\n",
    "                desc=\"Running tokenizer on validation dataset\",\n",
    "            )\n",
    "    \n",
    "\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8) \n",
    "\n",
    "\n",
    "    def post_processing_function(examples, features, predictions, stage=\"eval\"):\n",
    "        # Post-processing: we match the start logits and end logits to answers in the original context.\n",
    "        predictions = postprocess_qa_predictions(\n",
    "            examples=examples,\n",
    "            features=features,\n",
    "            predictions=predictions,\n",
    "            output_dir=training_args.output_dir,\n",
    "            prefix=stage,\n",
    "        )\n",
    "        formatted_predictions = [{\"id\": str(k), \"prediction_text\": v} for k, v in predictions.items()]\n",
    "\n",
    "        references = [{\"id\": str(ex[\"id\"]), \"answers\": ex[answer_column_name]} for ex in examples]\n",
    "        return EvalPrediction(predictions=formatted_predictions, label_ids=references)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"output_dir/{'aquad'}/\",\n",
    "        save_total_limit=2,\n",
    "        learning_rate=5e-5,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=3,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        report_to='none',\n",
    "        overwrite_output_dir=True,\n",
    "        fp16=True,\n",
    "        metric_for_best_model='eval_f1',\n",
    "    )\n",
    "\n",
    "    logger.info(\"Initializing Trainer...\")\n",
    "    trainer = QuestionAnsweringTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        eval_examples=raw_datasets['validation'],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        post_process_function=post_processing_function,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # logger.info(\"Starting training...\")\n",
    "    # trainer.train()\n",
    "\n",
    "    logger.info(\"Evaluating model on the test dataset...\")\n",
    "    test_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "    logger.info(\"\\n\\n=================SQuAD Dataset Results: \\n\")\n",
    "    logger.info(\"Test results: %s\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef796d6-6a14-49f1-a94e-fc7a8d5a3873",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

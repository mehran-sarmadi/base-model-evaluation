2024-08-24 10:49:29,662 - INFO - 





##################################################
################# Starting #################
##################################################






2024-08-24 10:49:29,662 - INFO - 


==================================Testing on Persian Dataset: ==================================


2024-08-24 10:49:29,662 - INFO - 

PQuAD

2024-08-24 10:49:29,662 - INFO - Loading dataset...
2024-08-24 10:49:37,886 - INFO - Loading tokenizer from /mnt/data/morteza-workspace/language-model/models/tokenizer/tookabert_tokenizer
2024-08-24 10:49:37,978 - INFO - Loading model from /mnt/data/mehran-workspace/models/tookabert-base
2024-08-24 10:49:40,398 - INFO - Tokenizing dataset...
2024-08-24 10:49:40,614 - INFO - Initializing Trainer...
2024-08-24 10:49:40,902 - INFO - Starting training...
2024-08-24 10:51:14,711 - INFO - Evaluating model on the test dataset...
2024-08-24 10:51:50,879 - INFO - 

=================SQuAD Dataset Results: 

2024-08-24 10:51:50,879 - INFO - Test results: {'eval_exact_match': 64.45466491458608, 'eval_f1': 82.50646049332337, 'eval_runtime': 9.1414, 'eval_samples_per_second': 665.978, 'eval_steps_per_second': 20.894, 'epoch': 3.0}
2024-08-24 10:51:50,883 - INFO - 

FarsTail

2024-08-24 10:51:50,883 - INFO - Loading dataset...
2024-08-24 10:51:55,519 - INFO - Loading tokenizer from /mnt/data/morteza-workspace/language-model/models/tokenizer/tookabert_tokenizer
2024-08-24 10:51:55,606 - INFO - Loading model from /mnt/data/mehran-workspace/models/tookabert-base
2024-08-24 10:51:57,677 - INFO - Tokenizing dataset...
2024-08-24 10:51:58,304 - INFO - Initializing Trainer...
2024-08-24 10:51:58,384 - INFO - Starting training...
2024-08-24 10:53:49,607 - INFO - Evaluating model on the test dataset...
2024-08-24 10:53:50,365 - INFO - 

=================FarsTail Dataset Results: 

2024-08-24 10:53:50,366 - INFO - Test results: {'eval_loss': 0.8902037143707275, 'eval_accuracy': 0.8446291560102301, 'eval_f1': 0.8440177596195525, 'eval_runtime': 0.7556, 'eval_samples_per_second': 2069.85, 'eval_steps_per_second': 64.848, 'epoch': 7.0}
2024-08-24 10:53:50,367 - INFO - 

DeepSentiPers

2024-08-24 10:53:50,367 - INFO - Loading dataset...
2024-08-24 10:53:54,165 - INFO - Mapping labels to ternary classification
2024-08-24 10:53:54,253 - INFO - Loading tokenizer from /mnt/data/morteza-workspace/language-model/models/tokenizer/tookabert_tokenizer
2024-08-24 10:53:54,338 - INFO - Loading model from /mnt/data/mehran-workspace/models/tookabert-base
2024-08-24 10:53:56,410 - INFO - Tokenizing dataset...
2024-08-24 10:53:59,145 - INFO - Initializing Trainer...
2024-08-24 10:53:59,225 - INFO - Starting training...
2024-08-24 10:55:30,025 - INFO - Evaluating model on the test dataset...
2024-08-24 10:55:32,874 - INFO - 

=================DeepSentiPers Dataset Results: 

2024-08-24 10:55:32,874 - INFO - Test results: {'eval_loss': 0.48251354694366455, 'eval_accuracy': 0.825782092772384, 'eval_f1': 0.792311280701945, 'eval_runtime': 2.8467, 'eval_samples_per_second': 651.287, 'eval_steps_per_second': 20.375, 'epoch': 3.0}
2024-08-24 10:55:32,875 - INFO - 
\ParsiNLU_Multi

2024-08-24 10:55:32,875 - INFO - Loading dataset...
2024-08-24 10:55:36,376 - INFO - Loading tokenizer from /mnt/data/morteza-workspace/language-model/models/tokenizer/tookabert_tokenizer
2024-08-24 10:55:36,472 - INFO - Loading model from /mnt/data/mehran-workspace/models/tookabert-base
2024-08-24 10:55:38,822 - INFO - Tokenizing dataset...
2024-08-24 10:55:38,982 - INFO - Initializing Trainer...
2024-08-24 10:55:39,065 - INFO - Starting training...
2024-08-24 10:56:06,583 - INFO - Evaluating model on the test dataset...
2024-08-24 10:56:07,516 - INFO - 

=================ParsiNLU Dataset Results: 

2024-08-24 10:56:07,516 - INFO - Test results: {'eval_loss': 1.965983510017395, 'eval_accuracy': 0.30857142857142855, 'eval_f1-weighted': 0.30766164482189723, 'eval_precision': 0.31081114863857257, 'eval_recall': 0.30857142857142855, 'eval_runtime': 0.9294, 'eval_samples_per_second': 1129.737, 'eval_steps_per_second': 35.506, 'epoch': 7.0}
